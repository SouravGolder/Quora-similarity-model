âœ… Fast Training (seconds/minutes per epoch)

Uses optimized GRU with CuDNN acceleration

Removed recurrent dropout

Reduced sequence length

Frozen embeddings

Larger batch size

ğŸ¯ Good Generalization

Dropout

L2 regularization

Batch normalization

Early stopping

ReduceLROnPlateau

Balanced class weights

ğŸ“ˆ Performance

The optimized model typically reaches:

Metric	Score
Accuracy	~0.73â€“0.74
F1 Score	~0.68
ROC AUC	~0.83

No overfitting or underfitting, stable validation curves.

ğŸ“„ Project Structure
â”œâ”€â”€ README.md
â”œâ”€â”€ train.csv
â”œâ”€â”€ model/
â”‚   â”œâ”€â”€ siamese_fast_gru.h5
â”‚   â””â”€â”€ tokenizer.pkl
â””â”€â”€ notebook.ipynb

ğŸ§  Approach Summary
1ï¸âƒ£ Data Preprocessing

Lowercasing

Removing punctuation

Stopword removal

Lemmatization

Tokenization

Padding sequences to fixed length

2ï¸âƒ£ Siamese GRU Architecture (Fast Version)

A Siamese network processes both questions through the same shared encoder:

Q1 â”€â–º [Shared Encoder] â”€â–º Vector 1
Q2 â”€â–º [Shared Encoder] â”€â–º Vector 2


We compute similarity using:

Absolute difference

Elementwise multiplication

Final prediction is generated through dense layers with dropout & L2 regularization.

* Key Design Decisions
* Why the model is much faster

GRU instead of LSTM â†’ fewer parameters

No recurrent dropout â†’ enables CuDNN acceleration

Smaller sequence length (25 tokens)

Embedding dimension = 50

Frozen embedding layer

Batch size increased (up to 512)

Training time dropped from 4 hours to <1 minute per epoch (GPU).

ğŸ” Why Overfitting is Solved

Lower model capacity

Dropout (0.3â€“0.4)

L2 regularization

Early stopping

Reduce learning rate on plateau

ğŸ“Š Interpreting Training Curves

If validation accuracy increases, then drops, then increases again â†’
this is called non-monotonic learning and is normal.

The model is healthy as long as:

Validation loss stabilizes or decreases

Validation AUC trends upward

ğŸ“¦ Future Improvements

You can extend the project by adding:

â‡ï¸ Pre-trained embeddings (GloVe/fastText)

â‡ï¸ BERT or Sentence-BERT Siamese architecture

â‡ï¸ Hard negative mining

Balanced classes

Results show validation and training curves closely aligned.
